% Jacobs Landscape Poster Version 1.1 (14/06/14)
\documentclass[final]{beamer}
\usepackage[scale=1.1]{beamerposter} % Use the beamerposter package for laying out the poster
\usetheme{confposter} % Use the confposter theme supplied with this template
\setbeamercolor{block title}{fg=ngreen,bg=white} % Colors of the block titles
\setbeamercolor{block body}{fg=black,bg=white} % Colors of the body of blocks
\setbeamercolor{block alerted title}{fg=white,bg=dblue!70} % Colors of the highlighted block titles
\setbeamercolor{block alerted body}{fg=black,bg=dblue!10} % Colors of the body of highlighted blocks

\fontfamily{lmss}
\newlength{\sepwid}
\newlength{\onecolwid}
\newlength{\twocolwid}
\newlength{\threecolwid}
\setlength{\paperwidth}{48in} % A0 width: 46.8in
\setlength{\paperheight}{36in} % A0 height: 33.1in
\setlength{\sepwid}{0.024\paperwidth} % Separation width (white space) between columns
\setlength{\onecolwid}{0.22\paperwidth} % Width of one column
\setlength{\twocolwid}{0.464\paperwidth} % Width of two columns
\setlength{\threecolwid}{0.708\paperwidth} % Width of three columns
\setlength{\topmargin}{-0.5in} % Reduce the top margin size

\usepackage{graphicx}  % Required for including images
\usepackage{booktabs} % Top and bottom rules for tables
\usepackage{pstricks,pst-plot,pst-text,pst-tree,pst-eps,pst-fill,pst-node,pst-math}

\title{Latent Dirichlet Allocation} % Poster title
\author{J\'er\^ome DOCK\`ES {\small{(\texttt{jerome@dockes.org})}}, Pascal LU  {\small{(\texttt{pascal.lu@centraliens.net})}} } % Author(s)
\institute{\'Ecole Normale Sup\'erieure de Cachan $-$ \today} % Institution

\begin{document}

\addtobeamertemplate{block end}{}{\vspace*{2ex}} % White space under blocks
\addtobeamertemplate{block alerted end}{}{\vspace*{2ex}} % White space under highlighted (alert) blocks

\setlength{\belowcaptionskip}{2ex} % White space under figures
\setlength\belowdisplayshortskip{2ex} % White space under equations

\begin{frame}[t] 
\begin{columns}[t]

%----------------------------------------------------------------------------------------
%	COLUMN 1
%----------------------------------------------------------------------------------------

\begin{column}{\sepwid}\end{column} % Empty spacer column

\begin{column}{\onecolwid} % The first column

\begin{alertblock}{Objectives}
We consider the problem of modeling text corpora. The goal is to find short descriptions of the members of a collection.
\medskip

Our work is mainly based on:
\nocite{*}
\small{\bibliographystyle{unsrt}\bibliography{sample}}
\end{alertblock}


\begin{block}{Notations}
\begin{itemize}
  \item $\mathcal{D} = \{d_{1},d_{2}, \ldots, d_{M}\}$ is a corpus.
  \item $\mathcal{V}$ is the vocabulary of size $V$.
  \item $k$ is the number of topics.
\end{itemize}

For a document $d \in \mathcal{D}$,
\begin{itemize}  
  \item $d = (w_1^{(d)}, \ldots, w_{N_d}^{(d)})$ represents the document $d$, where the $w_i^{(d)}$ are all distinct. $N_d$ is the number of \textbf{distinct} words in the document $d$.
  \item $w^{(d)}$ (\texttt{word$\_$incidences}) is a matrix containing the number of times each word in the vocabulary appears in the document. Size of $w^{(d)}$ = $N_d \times V$.
 \item $\theta^{(d)}$ is an array of size $k$, representing a probability density.
 \item $z^{(d)}$ is the set of topics : $z_{ni}^{(d)} =  1$ if the word $n$ is linked with the topic $i$. Size of $z^{(d)}$ $= N_d \times k$.
\end{itemize}
\end{block}


\begin{block}{Presentation of the model}
Latent Dirichlet allocation (LDA) is a generative probabilistic model of a corpus. 
\begin{itemize}
  \item Documents = random mixtures over latent topics,
  \item Topic = a distribution over words. 
\end{itemize} 

\medskip

\begin{itemize}
  \item \textbf{Input}: {corpus $\mathcal{D}$}
\end{itemize}

For {\emph{each document} $d \in \mathcal{D}$}{

\quad Choose $N \sim \textnormal{Poisson}(\xi)$

\quad Choose $\theta^{(d)} \sim \textnormal{Dir}(\alpha)$

\quad For {\emph{each of the $N$ words $w_n^{(d)}$}}{

\quad\quad Choose a topic $z_n^{(d)} \sim  \textnormal{Multinomial}(\theta^{(d)})$

\quad\quad Choose a word $w_n$ from $p(w_n |z_n^{(d)}, \beta)$, a multi-

\quad\quad nomial probability conditioned on $z_n^{(d)}$.
}
}
\end{block}

\end{column}


%----------------------------------------------------------------------------------------
% COLUMN 2
%----------------------------------------------------------------------------------------

\begin{column}{\sepwid}\end{column} % Empty spacer column
\begin{column}{\onecolwid} % The third column
\begin{block}{Generative model}
The goal is to determine:

\begin{itemize}
\item $\alpha$ = estimate of the parameter of the Dirichlet distribution which generates the parameter for the (multinomial) probability distribution over topics in the document. Size of $\alpha = k$.

We consider an exchangeable Dirichlet distribution: $\forall i \in \{1, \ldots, k\}$, $\alpha_i = \alpha$
\item $\beta$ is a matrix of size $k \times V$ which gives the estimated probability that a given topic will generate a certain word: $\beta_{ij}= p(w^j = 1 | z^i = 1)$.
\end{itemize}
\begin{align*}
p(d|\alpha, \beta) 
& = \int p(\theta^{(d)}|\alpha) \left( \prod_{n=1}^{N_{d}} p(w_n^{(d)}|\theta^{(d)}, \beta)\right) \text{d}\theta  \\
& = \int p(\theta^{(d)}|\alpha) \left( \prod_{n=1}^{N_{d}} \sum_{z_n^{(d)}} p(z_n^{(d)}|\theta^{(d)}) p(w_n^{(d)}|z_n^{(d)}, \beta)\right) \text{d}\theta
\end{align*}
\begin{figure}[ht!]
\begin{center}
\psset{xunit=3cm,yunit=3cm,algebraic=true,dimen=middle,dotstyle=o,dotsize=5pt 0,linewidth=0.8pt,arrowsize=3pt 2,arrowinset=0.25}
\begin{pspicture*}(-0.5,-0.1)(8,3.4)
\rput(0,1){\pscirclebox[linecolor=black,fillstyle=solid,fillcolor=blue]{\textcolor{white}{$\alpha_j$}}}
\rput(2,1){\pscirclebox{$\theta^{(d)}_j$}}
\rput(4,1){\pscirclebox{$z^{(d)}_{nj}$}}
\rput(6,1){\pscirclebox[linecolor=black,fillstyle=solid,fillcolor=yellow]{$w_{nj}^{(d)}$}}
\rput(6,3){\pscirclebox[linecolor=black,fillstyle=solid,fillcolor=blue]{\textcolor{white}{$\beta_{jv}$}}}
\pspolygon(3.25,0.25)(7.2,0.25)(7.2,1.75)(3.25,1.75)
\pspolygon(1.25,0)(7.75,0)(7.75,2)(1.25,2)
\psline{->}(0.27,1)(1.58,1)
\psline{->}(2.43,1)(3.58,1)
\psline{->}(4.44,1)(5.5,1)
\psline{->}(6,2.7)(6,1.5)
\rput(6.75,0.5){$N_d$}
\rput(7.5,0.25){$M$}
\end{pspicture*}
\end{center}
\end{figure}
\end{block}

\begin{block}{Variational inference}
$\Rightarrow$ Use Jensen's inequality to obtain a lower bound on the log likelihood.

For a document $d \in \mathcal{D}$:
 \begin{itemize}
  \item $\gamma^{(d)}$ is the variational parameter for the Dirichlet distribution. Size of $\gamma^{(d)}$ = $k$.
  \item $\phi^{(d)}$ is the variational parameter for the multinomial distribution. Size of $\phi^{(d)}$ = $N_d \times k$.
  
  $\phi_{ni}^{(d)}$ depends on the relation between the word in position $n$ of the document and the topic $i$ of the list of topics.
   \end{itemize}

$\Rightarrow$ Estimate $\gamma^{(d)}, \phi_{n}^{(d)}$ instead of $\theta^{(d)}$ and $z_n^{(d)}$.

\[ q(\theta^{(d)}, z^{(d)}|\gamma^{(d)}, \delta^{(d)}) = q(\theta^{(d)}|\gamma^{(d)}) \prod_{n=1}^{N_d} q(z_n^{(d)}|\phi_n^{(d)})\]

\begin{figure}
\begin{center}
\psset{xunit=3cm,yunit=3cm,algebraic=true,dimen=middle,dotstyle=o,dotsize=5pt 0,linewidth=0.8pt,arrowsize=3pt 2,arrowinset=0.25}
\begin{pspicture*}(-1,-0.1)(4,4)
\rput(0,3){\pscirclebox[linecolor=black,fillstyle=solid,fillcolor=green]{$\gamma^{(d)}_j$}}
\rput(0,1){\pscirclebox{$\theta^{(d)}_j$}}
\rput(2,3){\pscirclebox[linecolor=black,fillstyle=solid,fillcolor=green]{$\phi^{(d)}_{nj}$}}
\rput(2,1){\pscirclebox{$z^{(d)}_{nj}$}}
\pspolygon(1.25,0.25)(3.15,0.25)(3.15,3.75)(1.25,3.75)
\pspolygon(-0.9,0)(3.8,0)(3.8,4)(-0.9,4)
\psline{->}(0,2.5)(0,1.5)
\psline{->}(2,2.5)(2,1.5)
\rput(2.75,0.5){$N_d$}
\rput(3.5,0.25){$M$}
\end{pspicture*}
\end{center}
\end{figure}
\end{block}
\end{column}

%----------------------------------------------------------------------------------------
% COLUMN 3
%----------------------------------------------------------------------------------------

\begin{column}{\sepwid}\end{column} % Empty spacer column
\begin{column}{\onecolwid} % The third column

\begin{block}{Variational Inference Procedure (E-step for a document $d$)}
\begin{itemize}
  \item \textbf{Input}: a document $d$ defined by its \texttt{word$\_$incidences} ($w^{(d)}$), $\alpha, \beta$
  \item \textbf{Output}: $\gamma^{(d)}$, $\phi^{(d)}$
\end{itemize}

\medskip

Initialize $\phi_{ni}^{(d)} = \frac{1}{k}$ for all $i$ and $n$.

Initialize $\gamma_i^{(d)} = \alpha + \frac{1}{k}\sum_{n=1}^{N_d} w_n^{(d)}$ for all $i$.

While {\emph{the expected log-likelihood $L(\gamma^{(d)}, \phi^{(d)}, \alpha, \beta)$ for the document $d$ has not converged}}{

\quad For {$n=1\ldots N_d$}{

\quad\quad For {$i=1\ldots k$}{

\quad\quad\quad $\phi_{ni}^{(d)} = \beta_{iw_n^{(d)}}\exp(\Psi(\gamma_i^{(d)}))$

}

\quad Normalize $\phi_n^{(d)}$ to sum to $1$.
}

\quad $\gamma^{(d)} = \alpha + \sum_{n=1}^{N_d} w_n^{(d)} \phi_n^{(d)}$

\quad Update $L(\gamma^{(d)}, \phi^{(d)}, \alpha, \beta)$
}
\end{block}


\begin{block}{EM-algorithm}
\begin{itemize}
  \item \textbf{Input}: {Corpus $\mathcal{D}$, number of topics $k$}
  \item \textbf{Output}: $\alpha$, $\beta$
\end{itemize}

\medskip

For each $d \in \mathcal{D}$, compute $w^{(d)}$ (\texttt{word$\_$incidences}).

Initialize $\alpha$, $\beta$ and $\Sigma_{\gamma}$ = 0.

\medskip

While {\emph{the expected log-likelihood $L(\alpha, \beta)$ has not converged}: }{

\quad For {\emph{each} $d \in \mathcal{D}$}{

\quad\quad $(\gamma^{(d)}, \phi^{(d)})$ = \textbf{E-step}$(w^{(d)}, \alpha, \beta)$

\quad\quad Update $\beta \leftarrow \beta + (\phi^{(d)})^{\top}w^{(d)}$

\quad\quad Update $\Sigma_{\gamma} \leftarrow \Sigma_{\gamma} + \sum_{i=1}^k \Psi (\gamma_i^{(d)}) - \Psi\left( \sum_{j=1}^k \gamma_j^{(d)}\right)$

\quad\quad Update $L(\alpha, \beta) \leftarrow L(\alpha, \beta) +L(\gamma^{(d)}, \phi^{(d)}, \alpha, \beta)$
}

\quad Normalize $\beta$ 

\quad While {\emph{$\alpha$ has not converged}}{

\quad \quad $\alpha \leftarrow \alpha - \frac{L'(\alpha)}{L''(\alpha)}$ where \[ \begin{cases}L'(\alpha) = |\mathcal{D}| k \left[ \Psi\left( k \alpha \right) - \Psi(\alpha)\right] + \Sigma_{\gamma} \\
L''(\alpha) = |\mathcal{D}|k [k\Psi'(k\alpha) - \Psi' \left( \alpha\right)] \end{cases} \]

}
}
\end{block}

\begin{block}{Implementation issues}
\begin{itemize}
  \item Document and vocabulary preprocessing : remove redundant words.
  \item It is a non-convex optimization problem: estimates for $\alpha, \beta$ depend on their initialization.
  \item Under the exchangeable Dirichlet distribution assumption, the computation of $\alpha$ is much simpler. We initialized $\alpha > 0$ randomly. 
  \item $\beta$ was initialized randomly.
\end{itemize}
\end{block}
\end{column}

%----------------------------------------------------------------------------------------
% COLUMN 4
%----------------------------------------------------------------------------------------

\begin{column}{\sepwid}\end{column} % Empty spacer column

\begin{column}{\onecolwid} % The third column
\begin{block}{Results}
Tested on real data from the Reuters21578 database (\texttt{reut2-000.sgm}).

\begin{table}
\vspace{2ex}
\begin{footnotesize}
\begin{tabular}{ccccc}
\toprule
\textbf{Topic 1} & \textbf{Topic 2} & \textbf{Topic 3}  & \textbf{Topic 4} & \textbf{Topic 5}\\
\midrule
devices & prolonged & zestril & seasons & withdrawn \\
disk & council & annesthetic &  hotels & expiration  \\
megabyte & forum & hypertension & VMS & clearances \\
expandable & dissident & oth & Biltmore  & expire \\
megabytes & flying & statil & Marriott & Willemijn \\
equipped &  sparks & diabetic & rename & BV \\
monochrome &talks & complications & hotel & Rotterdam  \\
peripheral & outweighed & Barbara & 228 & licensed \\
color & accomplishments & definitive & DH  & NCR \\
\bottomrule
\end{tabular}
\end{footnotesize}
\caption{Results for 5 topics $(k=20)$}
\end{table}

\begin{figure}[ht!]
\begin{center}
\includegraphics[width=0.7\linewidth]{../img/k=20/log_likelihood_document_k=20.png}
\caption{Expected log-likelihood for a document ($k=20$)}
\end{center}
\end{figure}

\begin{figure}[ht!]
\begin{center}
\includegraphics[width=0.7\linewidth]{../img/k=20/log_likelihood_corpus_k=20.png}
\caption{Expected log-likelihood for the corpus ($k=20$)}
\end{center}
\end{figure}
\end{block}

\begin{block}{Conclusion}
\begin{itemize}
  \item LDA = a way to apply graphical models to information retrieval = group words in the same categories. 
  \item Key idea = variational inference. 
  \item Other interesting applications: biology (DNA sequence), content-based image retrieval$\ldots$
\end{itemize}
\end{block}

%\begin{block}{References}
%\nocite{*}
%\small{\bibliographystyle{unsrt}
%\bibliography{sample}\vspace{0.75in}}
%\end{block}

\setbeamercolor{block alerted title}{fg=black,bg=norange}
\setbeamercolor{block alerted body}{fg=black,bg=white}
%\begin{alertblock}{Contact Information}
%\begin{itemize}
%  \item \texttt{jerome\{at\}dockes.org}
%  \item \texttt{pascal.lu\{at\}centraliens.net}
%\end{itemize}
%\end{alertblock}
\end{column}

\end{columns} % End of all the columns in the poster
\end{frame} % End of the enclosing frame

\end{document}
