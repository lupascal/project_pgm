\documentclass[10pt]{article}
\usepackage[utf8]{inputenc} 
\usepackage{amssymb}
\usepackage{bm}
\usepackage{latexsym}
\usepackage{amsmath}
\usepackage{url}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
    bookmarks=true,         
    unicode=true,  
    colorlinks=true,       
    linkcolor=blue,          
    citecolor=blue,       
    filecolor=blue,      
    urlcolor=blue          
}
\DeclareMathOperator*{\argmin}{\arg\!\min}
\DeclareMathOperator*{\argmax}{\arg\!\max}
\usepackage{geometry}
\geometry{hmargin=2.5cm,vmargin=2.5cm}
\setlength{\parindent}{0pt}
\everymath{\displaystyle}
\renewcommand{\baselinestretch}{1.1}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{pstricks,pst-plot,pst-text,pst-tree,pst-eps,pst-fill,pst-node,pst-math}
\usepackage{caption}
\usepackage{subcaption}

\title{Latent Dirichlet allocation}
\date{\today}
\author{J\'er\^ome DOCK\`ES\footnote{\texttt{jerome@dockes.org}} \\ Pascal LU\footnote{\texttt{pascal.lu@student.ecp.fr}}}

\begin{document}

\maketitle

\section{Latent Dirichlet allocation: Theory}
\subsection{Presentation of the model}

We consider the problem of modeling text corpora. The goal is to find short descriptions of the members of a collection that enable efficient processing of large collections while preserving the essential statistical relationships that are useful for basic tasks such as classification, novelty detection, summarization, and similarity and relevance judgments.

\medskip

\textbf{Notations}
\begin{itemize}
\setlength\itemsep{-0.2em}
  \item $D = \{d_{1},d_{2}, \ldots, d_{M}\}$ is a corpus (collection of $M=|D|$ documents). We denote $|D|$ (\verb"num_docs") the number of documents.
  \item $V$ is the vocabulary. Its size is denoted $|V|$ (\verb"voc_size").
  \item The number of topics is denoted $k$ (\verb"num_topics").
\end{itemize}

For a document $d \in D$,
\begin{itemize}  
\setlength\itemsep{-0.2em}
  \item $d = (w_1, \ldots, w_{n(d)})$ represents the document $d$ of the corpus $D$, composed of a sequence of $n(d)$ words.
  \item $N_d$ (\verb"doc_size") is the number of \textbf{distinct}\footnote{For implementation issues} words in the document $d$.
  \item $w^{(d)}$ (\verb"word_incidences") is an matrix containing the number of times each word in the vocabulary appears in the document. The size of $w^{(d)}$ is the number of distinct words in document $d$ $\times$ vocabulary size ($|N_d| \times |V|$).
 %   $w_{nj}^{(d)} = \begin{cases} 1 & \text{if the word in position } n \text{ of the document is the word in position } j \text{ of the dictionary} \\ 0 & \text{otherwise} \end{cases}$
 \item $\theta^{(d)}$ is an array of size $k$, representing a probability density.
 \item $z^{(d)}$ is the set of topics : $z_{ni}^{(d)} =  1$ if the word $n$ is linked with the topic $i$. Hence, it is a matrix of size $|N_d| \times k$.
\end{itemize}

\medskip

Latent Dirichlet allocation (LDA) is a generative probabilistic model of a corpus. The main idea is that documents are represented as random mixtures over latent topics, where each topic is characterized by a distribution over words. 

\begin{algorithm}
\caption{Generative process}
\KwData{corpus $D$}
\Begin{
\For{\emph{each document} $d \in D$}{
Choose $N \sim \textnormal{Poisson}(\xi)$\;
Choose $\theta^{(d)} \sim \textnormal{Dir}(\alpha)$\;
\For{\emph{each of the $N$ words $w_n^{(d)}$}}{
Choose a topic $z_n^{(d)} \sim  \textnormal{Multinomial}(\theta^{(d)})$\;
Choose a word $w_n$ from $p(w_n |z_n^{(d)}, \beta)$, a multinomial probability conditioned on the topic $z_n$.
}
}
}
\end{algorithm}

\begin{figure}[ht!]
\begin{center}
\psset{xunit=1.0cm,yunit=1.0cm,algebraic=true,dimen=middle,dotstyle=o,dotsize=5pt 0,linewidth=0.8pt,arrowsize=3pt 2,arrowinset=0.25}
\begin{pspicture*}(-0.5,-0.3)(8,3.5)
\rput(0,1){\pscirclebox[linecolor=black,fillstyle=solid,fillcolor=blue]{\textcolor{white}{$\alpha$}}}
\rput(2,1){\pscirclebox{$\theta^{(d)}$}}
\rput(4,1){\pscirclebox{$z^{(d)}_n$}}
\rput(6,1){\pscirclebox[linecolor=black,fillstyle=solid,fillcolor=yellow]{$w_{n}^{(d)}$}}
\rput(6,3){\pscirclebox[linecolor=black,fillstyle=solid,fillcolor=blue]{\textcolor{white}{$\beta$}}}
\pspolygon(3.25,0.25)(7.2,0.25)(7.2,1.75)(3.25,1.75)
\pspolygon(1.25,0)(7.75,0)(7.75,2)(1.25,2)
\psline{->}(0.27,1)(1.58,1)
\psline{->}(2.43,1)(3.58,1)
\psline{->}(4.44,1)(5.5,1)
\psline{->}(6,2.7)(6,1.5)
\rput(6.75,0.5){$N_d$}
\rput(7.5,0.25){$M$}
\end{pspicture*}
\caption{Generative model}
\label{generative}
\end{center}
\end{figure}

The following parameters are introduced:
\begin{itemize}
  \item $\alpha$ (\verb"dirich_param") is an estimate of the parameter of the dirichlet distribution which generates the parameter for the (multinomial) probability distribution over topics in the document. The size of $\alpha$ is the number of topics, $k$. \textbf{We suppose that $\alpha = \alpha \mathbf{1}_{k}$.}
  
  \item $\beta$ (\verb"word_prob_given_topic") is a matrix of size (number of topics $\times$ vocabulary size $= k \times |V|$) which gives the (estimated) probability that a given topic will generate a certain word:
\[ \beta_{ij}= p(w^j = 1 | z^i = 1) \]
 \end{itemize}

LDA is based on the computation of the parameters $(\alpha, \beta)$. Once computed, we may the estimate the log-likelihood, with the probability $p(d|\alpha, \beta)$, defined for a document $d$:

\begin{align*}
p(d|\alpha, \beta) 
& = \int p(\theta^{(d)}|\alpha) \left( \prod_{n=1}^{N_{d}} p(w_n^{(d)}|\theta^{(d)}, \beta)\right) \text{d}\theta  \\
& = \int p(\theta^{(d)}|\alpha) \left( \prod_{n=1}^{N_{d}} \sum_{z_n^{(d)}} p(z_n|\theta^{(d)}) p(w_n^{(d)}|z_n^{(d)}, \beta)\right) \text{d}\theta  \\
& = \frac{\Gamma\left( \sum_i \alpha_i\right)}{\prod_i \Gamma(\alpha_i)} \int \left( \prod_{i=1}^k \theta_i^{\alpha_i - 1} \right) \left( \prod_{n=1}^N \sum_{i=1}^k \prod_{j=1}^V (\theta_i \beta_{ij})^{w_n^j} \right) \text{d}\theta
\end{align*}

\subsection{Inference and parameter estimation}

The basic idea of variational inference is to use Jensen's inequality to obtain an adjustable lower bound on the log likelihood.

An easy is to introduce, for each document $d$, the following latent variables:
 \begin{itemize}
\setlength\itemsep{-0.2em}
  \item $\gamma^{(d)}$ (\verb"var_dirich") the variational parameter for the dirichlet distribution. The size of $\gamma^{(d)}$ is the number of topics, $k$.
  \item $\phi^{(d)}$ (\verb"var_multinom") the variational parameter for the multinomial distribution The size of $\phi^{(d)}$ is (number of words in document $d$ $\times$ number of topics), $|N_d| \times k$.
  
  $\phi_{ni}^{(d)}$ depends on the relation between the word in position $n$ of the document and the topic $i$ of the list of topics.
   \end{itemize}

and to try to estimate them instead of $\theta^{(d)}$ and $z_n^{(d)}$. The conditional probability is:

\[ q(\theta^{(d)}, z^{(d)}|\gamma^{(d)}, \delta^{(d)}) = q(\theta^{(d)}|\gamma^{(d)}) \prod_{n=1}^{N_d} q(z_n^{(d)}|\phi_n^{(d)})\]

\begin{figure}[ht!]
\begin{center}
\begin{subfigure}{.5\textwidth}
\begin{center}
\psset{xunit=1.0cm,yunit=1.0cm,algebraic=true,dimen=middle,dotstyle=o,dotsize=5pt 0,linewidth=0.8pt,arrowsize=3pt 2,arrowinset=0.25}
\begin{pspicture*}(-0.5,-0.3)(8,3.5)
\rput(0,1){\pscirclebox[linecolor=black,fillstyle=solid,fillcolor=blue]{\textcolor{white}{$\alpha$}}}
\rput(2,1){\pscirclebox{$\theta^{(d)}$}}
\rput(4,1){\pscirclebox{$z^{(d)}_n$}}
\rput(6,1){\pscirclebox[linecolor=black,fillstyle=solid,fillcolor=yellow]{$w_{nj}^{(d)}$}}
\rput(6,3){\pscirclebox[linecolor=black,fillstyle=solid,fillcolor=blue]{\textcolor{white}{$\beta$}}}
\pspolygon(3.25,0.25)(7.2,0.25)(7.2,1.75)(3.25,1.75)
\pspolygon(1.25,0)(7.75,0)(7.75,2)(1.25,2)
\psline{->}(0.27,1)(1.58,1)
\psline{->}(2.43,1)(3.58,1)
\psline{->}(4.44,1)(5.5,1)
\psline{->}(6,2.7)(6,1.5)
\rput(6.75,0.5){$N_d$}
\rput(7.5,0.25){$M$}
\end{pspicture*}
\caption{Generative model}
\label{generative}
\end{center}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
\begin{center}
\psset{xunit=1.0cm,yunit=1.0cm,algebraic=true,dimen=middle,dotstyle=o,dotsize=5pt 0,linewidth=0.8pt,arrowsize=3pt 2,arrowinset=0.25}
\begin{pspicture*}(-1,-0.1)(4,4)
\rput(0,3){\pscirclebox[linecolor=black,fillstyle=solid,fillcolor=green]{$\gamma^{(d)}$}}
\rput(0,1){\pscirclebox{$\theta^{(d)}$}}
\rput(2,3){\pscirclebox[linecolor=black,fillstyle=solid,fillcolor=green]{$\phi^{(d)}_n$}}
\rput(2,1){\pscirclebox{$z^{(d)}_n$}}
\pspolygon(1.25,0.25)(3.15,0.25)(3.15,3.75)(1.25,3.75)
\pspolygon(-0.9,0)(3.8,0)(3.8,4)(-0.9,4)
\psline{->}(0,2.5)(0,1.5)
\psline{->}(2,2.5)(2,1.5)
\rput(2.75,0.5){$N_d$}
\rput(3.5,0.25){$M$}
\end{pspicture*}
\caption{Variational model}
\label{variational}
\end{center}
\end{subfigure}
\caption{Graphical model}
\end{center}
\end{figure}

\subsubsection{EM algorithm}

\begin{algorithm}
\caption{EM algorithm}
\KwData{Corpus $D$ of documents, number of topics}
\KwResult{\texttt{dirich$\_$param} ($\alpha$), \texttt{word$\_$prob$\_$given$\_$topic} ($\beta$)}
\Begin{
\For{\emph{each} $d \in D$}{
Compute \texttt{word$\_$incidences} ($w^{(d)}$).
}

Initialize \texttt{dirich$\_$param} ($\alpha$) with a uniform vector of size ``number of topics"\;
Initialize \texttt{word$\_$prob$\_$given$\_$topic} ($\beta$) \;

\bigskip

\While{not converged}{
\For{\emph{each} $d \in D$}{
\texttt{var$\_$dirich} ($\gamma^{(d)}$), \texttt{var$\_$multinom} ($\phi^{(d)}$) = apply \textbf{E-step} to each document $d$ given \texttt{word$\_$incidences} ($w^{(d)}$), \texttt{dirich$\_$param} ($\alpha$), \texttt{word$\_$prob$\_$given$\_$topic} ($\beta$)
}

\texttt{dirich$\_$param} ($\alpha$), \texttt{word$\_$prob$\_$given$\_$topic} ($\beta$) = Apply \textbf{M-step} given $\{$\texttt{word$\_$incidences} ($w^{(d)}$), \texttt{var$\_$dirich} ($\gamma^{(d)}$), \texttt{var$\_$multinom} ($\phi^{(d)}$), $d \in D\}$.
}
}
\end{algorithm}

\begin{algorithm}
\caption{E-step for a document $d$ (Variational Inference Procedure)}
\KwData{\texttt{word$\_$incidences} ($w^{(d)}$), \texttt{dirich$\_$param} ($\alpha$), \texttt{word$\_$prob$\_$given$\_$topic} ($\beta$)}
\KwResult{\texttt{var$\_$dirich} ($\gamma^{(d)}$), \texttt{var$\_$multinom} ($\phi^{(d)}$)}
\Begin{
Initialize $\phi_{ni}^{(d), 0} = \frac{1}{k}$ for all $i$ and $n$\;
Initialize $\gamma_i^{(d)} = \alpha_i + \frac{N}{k}$ for all $i$ \;
\While{not converged}{
\For{$n=1\ldots N$}{
\For{$i=1\ldots k$}{
$\phi_{ni}^{(d), t+1} = \beta_{iw_n}\exp(\Psi(\gamma_i^{(d), t}))$
}
normalize $\phi_n^{(d), t+1}$ to sum to $1$.
}
$\gamma^{(d), t+1} = \alpha + \sum_{n=1}^N \phi_n^{(d), t+1}$
}
}
\end{algorithm}

\begin{algorithm}
\caption{M-step}
\KwData{$\{$\texttt{word$\_$incidences} ($w^{(d)}$), \texttt{var$\_$dirich} ($\gamma^{(d)}$), \texttt{var$\_$multinom} ($\phi^{(d)}$), $d \in D\}$}
\KwResult{\texttt{dirich$\_$param} ($\alpha$), \texttt{word$\_$prob$\_$given$\_$topic} ($\beta$)}
\Begin{
$\beta_{ij} \propto \sum_{d \in D} \sum_{n=1}^{N_d} \phi_{ni}^{(d)} w_{nj}^{(d)}$

Until convergence:

\quad\quad $\alpha \leftarrow \alpha - H(\alpha)^{-1}g(\alpha)$ where $g (\alpha)= \left( \frac{\partial L}{\partial \alpha_i}\right)_i$ and $H(\alpha) = \left( \frac{\partial L}{\partial\alpha_i\partial\alpha_j}\right)_{ij}$

\[ \begin{cases} \frac{\partial L}{\partial \alpha_i} = |D| \left[ \Psi\left( \sum_{j=1}^k \alpha_j\right) - \Psi(\alpha_i)\right] + \sum_{d \in D} \left[\Psi (\gamma_i^{(d)}) - \Psi\left( \sum_{j=1}^k \gamma_j^{(d)}\right) \right] \\
 \frac{\partial L}{\partial \alpha_i\partial \alpha_j} = \delta_{ij} |D| \Psi'(\alpha_i) - \Psi' \left( \sum_{j=1}^k \alpha_j\right) \end{cases} \]
}
\end{algorithm}

We note that $H(\alpha) = \text{diag}(h) - 1z1^{\top}$ where:
\[ h = ( |D| \Psi'(\alpha_i))_i \quad \text{and} \quad z = \left(\Psi' \left( \sum_{p=1}^k \alpha_p\right)\right)_{i, j} \]

Then $(H^{-1}(\alpha)g(\alpha))_i = \frac{\frac{\partial L}{\partial \alpha_i} -c}{ \frac{\partial^2 L}{\partial \alpha_i^2}}$ where $c = \frac{\sum_{j=1}^k \frac{\frac{\partial L}{\partial \alpha_j}}{ \frac{\partial^2 L}{\partial \alpha_j^2}}}{z^{-1}+\sum_{j=1}^k \left( \frac{\partial^2 L}{\partial \alpha_i^2}\right)^{-1}}$.

\subsubsection{Convergence criterion: expected log-likelihood}

\begin{align*}
L(\gamma, \phi, \alpha, \beta)
& = \log \Gamma\left( \sum_{j=1}^k\alpha_j \right) - \sum_{i=1}^k \log \Gamma(\alpha_i) + \sum_{i=1}^k (\alpha_i- 1) \left( \Psi(\gamma_i) - \Psi\left( \sum_{j=1}^k \gamma_j \right) \right) \\
&+ \sum_{n=1}^N\sum_{i=1}^k \phi_{ni} \left( \Psi(\gamma_i) - \Psi\left( \sum_{j=1}^k \gamma_j \right) \right) \\
&+ \sum_{n=1}^N \sum_{i=1}^k \sum_{j=1}^V \phi_{ni} w_n^j \log \beta_{ij} \\
& - \log \Gamma \left( \sum_{j=1}^k \gamma_j \right) + \sum_{i=1}^k \log \Gamma(\gamma_i) - \sum_{i=1}^k(\gamma_i - 1) \left( \Psi(\gamma_i) - \Psi\left( \sum_{j=1}^k \gamma_j \right)\right) \\
&- \sum_{n=1}^N \sum_{i=1}^k \phi_{ni}\log\phi_{ni} 
\end{align*}

\section{Implementation issues}


\subsection{Initialization and computation of $\alpha$}

\begin{itemize}
\setlength\itemsep{-0.2em}
  \item We need to initialize of $\alpha$ and $\beta$ before starting the EM-algorithm. The initialization step still remains a problem.
  \item The computation of $\alpha$ uses a Newton-Raphson algorithm ``$\alpha \leftarrow \alpha - H(\alpha)^{-1}g(\alpha)$". However, the convergence of $\alpha$ depends on the initialization of $\alpha$.
\end{itemize}

\subsection{Initialization of $\beta$}

The computation of $\beta$ was simplified. It is essentially based on the sum of $(\phi^{(d)})^{\top}w^{(d)}$ for $d \in D$.

\section{Results}



\section{Bibliography}




\end{document}

build voc







given a corpus of documents $D = \{\mathbf{w}_1,\mathbf{w}_2, \ldots, \mathbf{w}_M\}$

\[ \ell(\alpha, \beta) = \sum_{d = 1}^M \log p(\mathbf{w}_d| \alpha, \beta) \]

where:
\[ p(\mathbf{w}| \alpha, \beta) = \frac{\Gamma\left( \sum_i \alpha_i\right)}{\prod_i \Gamma(\alpha_i)} \int \left( \prod_{i=1}^k \theta_i^{\alpha_i - 1} \right) \left( \prod_{n=1}^N \sum_{i=1}^k \prod_{j=1}^V (\theta_i \beta_{ij})^{w_n^j} \right) \text{d}\theta \]
